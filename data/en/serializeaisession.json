{
    "engines": {"lucee": {
        "docs": "https://docs.lucee.org/reference/functions/SerializeAISession.html",
        "minimum_version": ""
    }},
    "name": "SerializeAISession",
    "description": "Serializes an AI session to a JSON string that includes its configuration and conversation history. The serialized content includes:\n- Configuration settings (temperature, limits, timeouts, system message)\n- Complete conversation history with questions and answers\nThis function is useful for:\n- Persisting AI sessions\n- Debugging AI conversations\n- Sharing or transferring AI session data\n- Storing AI conversation history\nUse `LoadAISession()` to restore a serialized session.",
    "syntax": "SerializeAISession( session )",
    "returns": "string",
    "type": "function",
    "params": [
        {
            "name": "session",
            "description": "The AI session object returned by LuceeCreateAISession(). This session maintains the conversation history and configuration settings like temperature and system message.",
            "aliases": "aiSession",
            "type": "any",
            "required": true
        },
        {
            "name": "maxlength",
            "description": "Specifies the maximum number of conversation exchanges to retain in the serialized output. \nWhen specified, the function preserves the most recent exchanges up to this limit, truncating older exchanges while maintaining the session configuration. \nThis helps control memory usage and storage requirements for long-running conversations without losing recent context. \nWhen used with condense=true, maxlength is applied before condensation. If not specified, all conversation history is retained.",
            "aliases": "length",
            "type": "numeric",
            "required": false
        },
        {
            "name": "condense",
            "description": "Controls whether the conversation history should be condensed before serialization. When set to true, the function will intelligently summarize older parts of the conversation while preserving essential context. This reduces the overall token count and storage requirements while maintaining the important elements of the conversation history. Default is false, which preserves the complete conversation history.",
            "aliases": "summarize",
            "type": "boolean",
            "required": false
        }
    ]
}